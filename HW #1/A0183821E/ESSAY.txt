1. Yes, we would expect a token-based ngram model to perform better. This is because there is greater distinction between the words of a language. For the character ngram models, we take ngrams into account which makes little to no grammatical sense for the language. However, we would require a much larger training set and the resources required (in terms of space) will be much higher due to the number of possible token ngrams.

2. If we were given more training data, it is likely that we would be able to predict the language of a unlabelled data with greater accuracy. However, if we were given more data only for a specific language (say, Indonesia), if there were overlap of ngrams (as we see here for Indonesian and Malaysian), then there will be greater bias towards Indonesian. This will lead to inaccuracies in the prediction.

3. I think that this might give us more accuracy for certain languages. In particular, this would potentially help reduce outliers from our training data, such as texts in all capital letters. Of course, it depends on the context of the model. If we wanted to examine LM of text messages, then strings with all capitalised letters may be much more common than, say, an academic journal. 

4. I think that as n increases, so will the accuracy of our predictions of our LM. Imagine using a unigram model. All romanised languages would then have the same vocabulary, and then the only thing affecting the prediction is the size of the training set for the languages. However, if we make the ngram size too large, we risk making it too specialised towards our training data, leading to inaccuracies in prediction.